{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sowmiyakarthickdeepan/UnSupervised-Netflix-Movies-and-TV-Shows-Clustering/blob/projects/NETFLIX_MOVIES_AND_TV_SHOWS_CLUSTERING_SOWMIYA_KARTHICK_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  NETFLIX MOVIES AND TV SHOWS CLUSTERING\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix is a popular streaming service that provides its subscribers with a wide range of movies,TV shows,documentaries,and original content to watch on demand. The company was founded in 1997 as a DVD rental service but later pivoted to an online streaming model in 2007. Since then, Netflix has grown into one of the most popular streaming services globally,with over 200 million subscribers in more than 190 countries as 2021.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* https://github.com/sowmiyakarthickdeepan/UnSupervised-Netflix-Movies-and-TV-Shows-Clustering/blob/projects/NETFLIX_MOVIES_AND_TV_SHOWS_CLUSTERING_SOWMIYA_KARTHICK.ipynb\n",
        "* https://github.com/sowmiyakarthickdeepan/UnSupervised-Netflix-Movies-and-TV-Shows-Clustering/blob/projects/NETFLIX_MOVIES_AND_TV_SHOWS_CLUSTERING_SOWMIYA_KARTHICK_1.ipynb\n",
        "* https://github.com/sowmiyakarthickdeepan/UnSupervised-Netflix-Movies-and-TV-Shows-Clustering/blob/projects/NETFLIX_MOVIES_AND_TV_SHOWS_CLUSTERING_SOWMIYA_KARTHICK_2.ipynb\n",
        "* https://github.com/sowmiyakarthickdeepan/UnSupervised-Netflix-Movies-and-TV-Shows-Clustering/blob/projects/NETFLIX_MOVIES_AND_TV_SHOWS_CLUSTERING_SOWMIYA_KARTHICK_3.ipynb"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contains tv shows and movies available on Netflix as of 2019. The data is collected from Flixable which is a third-party  Netflix search engine. In 2018,they released an interesting report which shows that the number of tv shows on Netflix has nearly tripled since 2010. The streaming service's number of movies has decreased by more than 2000 titles since 2010,while its number of tv shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same datset.\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "%matplotlib inline\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import string\n",
        "string.punctuation\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "import warnings;warnings.filterwarnings('ignore')\n",
        "import warnings;warnings.simplefilter('ignore')\n",
        "\n",
        "# Other Libraries\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import re, string, unicodedata\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EiQp_0XJS_Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset Info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(dataset[dataset.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Missing Values/Null Values Count\n",
        "print(dataset.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#total null values present in the data\n",
        "dataset.isnull().sum().sum()"
      ],
      "metadata": {
        "id": "nUKlF-tt0MUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(dataset.isnull(),cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Dataset contains 7787 observations with 12 features. It has some null values present in director, cast, country, data_added and rating.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "dataset.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "GyKGBtBx2AJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Show_id : Unique id for every movies/tv shows\n",
        "\n",
        "* Type : identifier-A movie or Tv show\n",
        "\n",
        "* Title : Title of the movie/show\n",
        "\n",
        "* Director : Director of the show\n",
        "\n",
        "* Cast : Actors involved in the show\n",
        "\n",
        "* Country : Country of production\n",
        "\n",
        "* Date_added : Date is what added on netflix\n",
        "\n",
        "* Release_Year : Actual release Year of the show\n",
        "\n",
        "* Rating : TV rating of the show\n",
        "\n",
        "* Duration : Total duration in minutes or number of seasons\n",
        "\n",
        "* Listed_In : Genre\n",
        "\n",
        "* Description : The summary description\n"
      ],
      "metadata": {
        "id": "FR0mJsSu2rLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values of each variable"
      ],
      "metadata": {
        "id": "8BFuVrcH8gYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check Unique Values of each variable\n",
        "variables_data=dataset.columns.to_list()\n",
        "for item in variables_data:\n",
        "  print(\"The Unique Values of\",item,\"are\",dataset[item].unique())"
      ],
      "metadata": {
        "id": "g3utlRhp9ICl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy of the data\n",
        "df=dataset.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# handling null values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "7Gs46q8T_ZE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling the null values of director and cast\n",
        "df.fillna( {'Director': 'name absent', 'cast': 'name missing' } ,inplace=True)\n",
        "\n",
        "#filling the null values of rating and country on their mode\n",
        "df[\"country\"].fillna(df[\"country\"].mode()[0],inplace=True)\n",
        "df[\"rating\"].fillna(df[\"rating\"].mode()[0],inplace=True)\n",
        "\n",
        "#dropping the observation with null values present in date added\n",
        "df=df[df[\"date_added\"].notna()]"
      ],
      "metadata": {
        "id": "B9GZJVHD_qWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n"
      ],
      "metadata": {
        "id": "tfau5T5Qqogq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# first and last date on which a show was added on Netflix\n",
        "df.date_added.min(),df.date_added.max()"
      ],
      "metadata": {
        "id": "8fUnER7DquXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding new attributes month and year of date added\n",
        "\n",
        "df['month_added'] = df['date_added'].dt.month\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "df.drop('date_added', axis=1, inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "CTmNeZs6q2tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "4B4nScoVEnFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the rating  value\n"
      ],
      "metadata": {
        "id": "qPvpRLbXIQuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rating value counts\n",
        "df[\"rating\"].value_counts()"
      ],
      "metadata": {
        "id": "LJKyMjAKInMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Made a copy of the original dataset as df.\n",
        "\n",
        "* Checked the null value counts of each column.\n",
        "\n",
        "* Handled the null values by filling in different values based on the column values. Like, the country and rating column has been filled by its mode. The Director and Cast are filled by name absent and name missig. Apart from that data_added has also some null values so I picked the data where the nan value of data_added was not present and assigned them to df.\n",
        "\n",
        "* Converted the rating into an understandable format.\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Movie and TV shows Pie chart\n"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(7,7))\n",
        "df.type.value_counts().plot(kind='pie',autopct='%1.2f%%')\n",
        "plt.ylabel('')\n",
        "plt.title('Movies and TV Shows in the dataset')\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart expresses a part-to-whole relationship in your data. It's easy to explain the percentage comparison through area covered in a circle with different colors. Where differenet percentage comparison comes into action pie chart is used frequently. So, I used Pie chart and which helped me to get the percentage comparision of the dependant variable."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A pie chart, sometimes called a circle chart, is a way of summarizing a set of nominal data or displaying the different values of a given variable\n",
        "\n",
        "* In this pie plot there are more movies 69.14% than TV shows 30.86% in the dataset."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Yes! the production house should more focus on quality movies because there is high competition in the market.\n",
        "\n",
        "* TV Shows are less in numbers hence good opportunity for business.\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 - Top 10 Ditrectorsin TV shows"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "df[~(df['director']=='Unknown')].director.value_counts().nlargest(10).plot(kind='bar')\n",
        "plt.title('Top 10 directors by number of shows ')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts show the frequency counts of values for the different levels of a categorical or nominal variable. Sometimes, bar charts show other statistics, such as percentages.\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Barplot (or Barchart) is one of the most common types of graphic. It shows the relationship between a numeric and a categoric variable. Each entity of the categoric variable is represented as a bar. The size of the bar represents its numeric value."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - Top 10 countries with highest number of movies/TV shows"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "df[~(df['country']=='Unknown')].country.value_counts().nlargest(10).plot(kind='bar')\n",
        "plt.title(' Top 10 countries with the highest number of shows')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts show the frequency counts of values for the different levels of a categorical or nominal variable. Sometimes, bar charts show other statistics, such as percentages."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Barplot (or Barchart) is one of the most common types of graphic. It shows the relationship between a numeric and a categoric variable. Each entity of the categoric variable is represented as a bar. The size of the bar represents its numeric value.\n",
        "\n",
        "* United States are the top one country of highest number movies / TV shows in the dataset.\n",
        "\n",
        "* Australia are the top 10 highest number movies / TV shows in the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - TV show / Movie Release year"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df['release_year'])\n",
        "plt.title('distribution by released year')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In statistics, a histogram is a graphical representation of the distribution of data. The histogram is represented by a set of rectangles, adjacent to each other, where each bar represent a kind of data."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this histogram we found release_year 2000 to 2020 more the the movie / tv show was released"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 - Rating"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.countplot(x='rating',data=df)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Countplot() method is used to Show the counts of observations in each categorical bin using bars."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In this plot we found all rating count.\n",
        " * TV-MA is the highest rating\n",
        " * Second highest is TV-14\n",
        " * Third highest is TV-PG rating."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 - Top 10 Genres"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "df.listed_in.value_counts().nlargest(10).plot(kind='bar')\n",
        "plt.title('Top 10 genres')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar graph is a graphical representation of data in which we can highlight the category with particular shapes like a rectangle. The length and heights of the bar chart represent the data distributed in the dataset. In a bar chart, we have one axis representing a particular category of a column in the dataset and another axis representing the values or counts associated with it. Bar charts can be plotted vertically or horizontally. A vertical bar chart is often called a column chart."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In this bar plot we found Dramas are large value_counts of shows and movies and Comedies are the second large value_counts of shows and Movies."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 - Distribution of Age rating"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "rating_map = {'TV-MA':'Adults',\n",
        "              'R':'Adults',\n",
        "              'PG-13':'Teens',\n",
        "              'TV-14':'Young Adults',\n",
        "              'TV-PG':'Older Kids',\n",
        "              'NR':'Adults',\n",
        "              'TV-G':'Kids',\n",
        "              'TV-Y':'Kids',\n",
        "              'TV-Y7':'Older Kids',\n",
        "              'PG':'Older Kids',\n",
        "              'G':'Kids',\n",
        "              'NC-17':'Adults',\n",
        "              'TV-Y7-FV':'Older Kids',\n",
        "              'UR':'Adults'}\n",
        "\n",
        "df['rating'].replace(rating_map, inplace = True)\n",
        "df['rating'].unique()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x='rating',data=df)\n",
        "plt.title('Number of shows on Netflix for different age groups')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rWCCGcqXz5_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot() method is used to Show the counts of observations in each categorical bin using bars."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this count plot we found that Number of shows on Netflix for different age groups.\n",
        "* Adults are the highest number of shows.\n",
        "* Second highest is Young Adults.\n",
        "* Third highest is older Kids shows."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 - Bivariate Analysis"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Number of movies and TV shows added over the years\n",
        "plt.figure(figsize=(10,5))\n",
        "p = sns.countplot(x='year_added',data=df, hue='type')\n",
        "plt.title('Number of movies and TV shows added over the years')\n",
        "plt.xlabel('')\n",
        "for i in p.patches:\n",
        "  p.annotate(format(i.get_height(), '.0f'), (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bivariate plot graphs the relationship between two variables that have been measured on a single sample of subjects. Such a plot permits you to see at a glance the degree and pattern of relation between the two variables."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this graph we found that Number of movies and TV shows added over the years.\n",
        "* 2018 year 1255 movies and 430 TV shows are there.\n",
        "* 2019 are the 1497 movies and 656 TV shows are there.\n",
        "* 2020 year 1312 movies and 697 TV shows are there ."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 -  Shows Release year"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Number of shows released each year since 2008\n",
        "order = range(2008,2022)\n",
        "plt.figure(figsize=(10,5))\n",
        "p = sns.countplot(x='release_year',data=df, hue='type',\n",
        "                  order = order)\n",
        "plt.title('Number of shows released each year since 2008 that are on Netflix')\n",
        "plt.xlabel('')\n",
        "for i in p.patches:\n",
        "  p.annotate(format(i.get_height(), '.0f'), (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A bivariate plot graphs the relationship between two variables that have been measured on a single sample of subjects. Such a plot permits you to see at a glance the degree and pattern of relation between the two variables."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this graph we found that Number of shows released each year since 2008 that are on Netflix.see this graph Before 2019 Movies are highest number of released but 2020 and 2021 TV shows are the highest number of released."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - Top 10 genres for TV Shows"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "df[df['type']=='TV Show'].listed_in.value_counts().nlargest(10).plot(kind='bar')\n",
        "plt.title('Top 10 genres for TV Shows')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A bar plot shows catergorical data as rectangular bars with the height of bars proportional to the value they represent. It is often used to compare between values of different categories in the data."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In this graph we found Top 10 genres for movies. International TV Shows are the highest number of genres type of TV shows and Crime TVShows and Kid's Shows are approximately same for generes type of TV shows."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Wordcloud for movie"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# Building a wordcloud for the movie descriptions\n",
        "comment_words = ''\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "# iterate through the csv file\n",
        "for val in df.description.values:\n",
        "\n",
        "    # typecaste each val to string\n",
        "    val = str(val)\n",
        "\n",
        "    # split the value\n",
        "    tokens = val.split()\n",
        "\n",
        "    # Converts each token into lowercase\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = tokens[i].lower()\n",
        "\n",
        "    comment_words += \" \".join(tokens)+\" \"\n",
        "\n",
        "wordcloud = WordCloud(width = 700, height = 700,\n",
        "                background_color ='white',\n",
        "                stopwords = stopwords,\n",
        "                min_font_size = 10).generate(comment_words)\n",
        "\n",
        "\n",
        "# plot the WordCloud image\n",
        "plt.figure(figsize = (10,5), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word clouds or tag clouds are graphical representations of word frequency that give greater prominence to words that appear more frequently in a source text. The larger the word in the visual the more common the word was in the document(s)."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this graph we found life,family,love,friend,world,new,find words are many time uses."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Top 15 countries contents"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "#Analysing top 15 countries with most content\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x=df['country'],order=df['country'].value_counts().index[0:15],hue=df['type'])\n",
        "plt.xticks(rotation=50)\n",
        "plt.title('Top 15 countries with most contents', fontsize=15, fontweight='bold')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Countplot() method is used to Show the counts of observations in each categorical bin using bars.\n",
        "\n"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this count plot we found that top 15 countries on Netflix for TV Shows and Movies.\n",
        "* United states has the highest number of content on the netflix.\n",
        "* second state with highest number of content is India.\n",
        "* Philippiness has only movie content.\n"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Wordcloud for Description"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Create a string to store all the words.\n",
        "comment_words = ''\n",
        "\n",
        "# Remove the stopwords.\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "# iterate through the column.\n",
        "for val in df.description:\n",
        "\n",
        "    # typecaste each val to string\n",
        "    val = str(val)\n",
        "\n",
        "    # split the value\n",
        "    tokens = val.split()\n",
        "\n",
        "    # Converts each token into lowercase\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = tokens[i].lower()\n",
        "\n",
        "    comment_words += \" \".join(tokens)+\" \"\n",
        "\n",
        "wordcloud = WordCloud(width = 1000, height = 500,\n",
        "                background_color ='white',\n",
        "                stopwords = stopwords,\n",
        "                min_font_size = 10,\n",
        "                max_words = 1000,\n",
        "                colormap = 'gist_heat').generate(comment_words)\n",
        "\n",
        "# plot the WordCloud image\n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.title('Most used words in description', fontsize = 30, pad=25)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word clouds or tag clouds are graphical representations of word frequency that give greater prominence to words that appear more frequently in a source text. The larger the word in the visual the more common the word was in the document(s)."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most occurring words in the description of the tv shows and movies are Family, Friend, Love, Life, Woman, Man."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "month_year_df = df.groupby('year_added')['month_added'].value_counts().unstack().fillna(0).T\n",
        "\n",
        "plt.figure(figsize=(11,8))\n",
        "sns.heatmap(month_year_df, linewidths=0.025, cmap=\"YlGnBu\")\n",
        "plt.title(\"Content Heatmap\")\n",
        "plt.ylabel(\"Month\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses. The range of correlation is [-1,1]."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above heatmap, we can see that around 2014 is when Netflix began to increase their content count. We can see over the years and months, Netflix continues to slowly increase the amount of content that is being added into their platform. We can see in 2020, the data stops at January since that is the latest month available in the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the dataset columns are all in string format and pair plots need numerical data to provide useful visualizations, we are unable to make a pair plot using the dataset."
      ],
      "metadata": {
        "id": "X6RxXi3Nn9RH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothetical Statement  1:\n",
        "* Null Hypothesis: There is no significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "* Alternative Hypothesis: There is a significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "Hypothetical Statement 2:\n",
        "\n",
        "* Null Hypothesis: The average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n",
        "* Alternative Hypothesis: The average duration of TV shows added in the year 2020 on Netflix is significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n",
        "Hypothetical Statement 3:\n",
        "\n",
        "* Null Hypothesis: The proportion of TV shows added on Netflix that are produced in the United States is not significantly different from the proportion of movies added on Netflix that are produced in the United States.\n",
        "\n",
        "* Alternative Hypothesis: The proportion of TV shows added on Netflix that are produced in the United States is significantly different from the proportion of movies added on Netflix that are produced in the United States."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null Hypothesis: There is no significant difference in the proportion ratings of drama movies and comedy movies on Netflix.\n",
        "\n",
        "* Alternative Hypothesis: There is a significant difference in the proportion ratings of drama movies and comedy movies on Netflix."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from statsmodels.stats.proportion import proportions_ztest  #------> This function is used to perform z test of proportion.\n",
        "\n",
        "# Subset the data to only include drama and comedy movies\n",
        "subset = df[df['type'].str.contains('Dramas') | df['type'].str.contains('Comedies')]\n",
        "\n",
        "# Calculate the proportion of drama and comedy movies\n",
        "drama_prop = len(subset[subset['type'].str.contains('Dramas')])\n",
        "comedy_prop = len(subset[subset['type'].str.contains('Comedies')])\n",
        "\n",
        "# Set up the parameters for the z-test\n",
        "count = [int(drama_prop * len(subset)), int(comedy_prop * len(subset))]\n",
        "nobs = [len(subset), len(subset)]\n",
        "alternative = 'two-sided'\n",
        "\n",
        "# Perform the z-test\n",
        "z_stat, p_value = proportions_ztest(count=count, nobs=nobs, alternative=alternative)\n",
        "print('z-statistic: ', z_stat)\n",
        "print('p-value: ', p_value)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results of the z-test\n",
        "if p_value < alpha:\n",
        "    print(f\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(f\"Fail to reject the null hypothesis.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test we have used to obtain the P-value is the z-test for proportions.\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The z-test for proportions was chosen because we are comparing the proportions of two categorical variables (drama movies and comedy movies) in a sample. The null hypothesis and alternative hypothesis are about the difference in proportions, and we want to determine if the observed difference in proportions is statistically significant or not. The z-test for proportions is appropriate for this situation because it allows us to compare two proportions and calculate the probability of observing the difference we see in our sample if the null hypothesis were true."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null Hypothesis: The average duration of TV shows added in the year 2020 on Netflix is not significantly different from the average duration of TV shows added in the year 2021.\n",
        "\n",
        "* Alternative Hypothesis: The average duration of TV shows added in the year 2020 on Netflix is significantly different from the average duration of TV shows added in the year 2021."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "# Filter TV shows for the year 2018 and 2019\n",
        "tv_shows_2018 = df[(df[\"type\"] == \"TV Show\") & (df[\"release_year\"] == 2018)].copy()\n",
        "tv_shows_2019 = df[(df[\"type\"] == \"TV Show\") & (df[\"release_year\"] == 2019)].copy()\n",
        "\n",
        "# Convert duration to numeric values\n",
        "tv_shows_2018[\"duration\"] = pd.to_numeric(tv_shows_2018[\"duration\"].str.extract('(\\d+)')[0], errors=\"coerce\")\n",
        "tv_shows_2019[\"duration\"] = pd.to_numeric(tv_shows_2019[\"duration\"].str.extract('(\\d+)')[0], errors=\"coerce\")\n",
        "\n",
        "tv_shows_2018 = tv_shows_2018.dropna()\n",
        "tv_shows_2019 = tv_shows_2019.dropna()\n",
        "\n",
        "# Perform the two-sample t-test\n",
        "t_stat, p_value = ttest_ind(tv_shows_2018[\"duration\"], tv_shows_2019[\"duration\"], equal_var=False)\n",
        "alpha = 0.05\n",
        "\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test performed to obtain the P-value is the two-sample independent t-test.\n"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample independent t-test was chosen in this case because we are comparing the means of two independent groups of data: the duration of TV shows released in the year 2018 and the duration of TV shows released in the year 2019. The t-test is appropriate for comparing the means of two groups when the data is continuous and normally distributed. In this scenario, we want to determine if there is a significant difference in the average duration of TV shows between the two years. By calculating the t-statistic and the corresponding P-value, the test helps us evaluate whether the observed difference in durations is statistically significant or if it could be due to random chance. The null hypothesis in this case is that there is no difference in the mean durations between the two years, and the P-value will indicate whether we can reject this null hypothesis or not at a chosen significance level (alpha)."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Null Hypothesis: The average duration of TV Shows released in the first 6 months is the same as those released in the last 6 months.\n",
        "\n",
        "* Alternate Hypothesis: The average duration of TV Shows released in the first 6 months is different from those released in the last 6 months."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Filter TV Shows for the first 6 months and last 6 months\n",
        "tv_shows_first_6_months = df[(df[\"type\"] == \"TV Show\") & (df[\"release_year\"] == 2020) & (df.isin([\"January\", \"February\", \"March\", \"April\", \"May\", \"June\"]))].copy()\n",
        "tv_shows_last_6_months = df[(df[\"type\"] == \"TV Show\") & (df[\"release_year\"] == 2020) & (df.isin([\"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]))].copy()\n",
        "\n",
        "# Convert duration to numeric values\n",
        "tv_shows_first_6_months[\"duration\"] = pd.to_numeric(tv_shows_first_6_months[\"duration\"].str.extract('(\\d+)')[0], errors=\"coerce\")\n",
        "tv_shows_last_6_months[\"duration\"] = pd.to_numeric(tv_shows_last_6_months[\"duration\"].str.extract('(\\d+)')[0], errors=\"coerce\")\n",
        "\n",
        "# Remove NaN values\n",
        "tv_shows_first_6_months = tv_shows_first_6_months.dropna()\n",
        "tv_shows_last_6_months = tv_shows_last_6_months.dropna()\n",
        "\n",
        "# Perform the two-sample t-test\n",
        "t_stat, p_value = ttest_ind(tv_shows_first_6_months[\"duration\"], tv_shows_last_6_months[\"duration\"], equal_var=False)\n",
        "alpha = 0.05\n",
        "\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test performed to obtain the P-value is the two-sample independent t-test."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample independent t-test was chosen in this case because we are comparing the means of two independent groups of data: the duration of TV shows released in the first 6 months of the year 2020 and the duration of TV shows released in the last 6 months of the same year. The t-test is appropriate for comparing the means of two groups when the data is continuous and normally distributed. Here, we want to investigate whether there is a statistically significant difference in the average duration of TV shows between the two periods. By calculating the t-statistic and the corresponding P-value, the test allows us to assess whether the observed difference in durations is likely due to a true difference or simply due to random variation. The null hypothesis states that there is no difference in the mean durations between the two time periods, and the P-value helps us determine whether we can reject this null hypothesis or not, given a chosen significance level (alpha)."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "missing_df = df.copy()\n",
        "\n",
        "def missing_value_check(df):\n",
        "  number_missing = missing_df.isnull().sum()\n",
        "  missing_value_df = pd.DataFrame({'column_name' : missing_df.columns,\n",
        "                                  'number_missing': number_missing})\n",
        "  return missing_value_df.sort_values('number_missing', ascending = False)\n",
        "missing_value_check(missing_df)\n",
        "\n",
        "# Checking again\n",
        "missing_value_check(missing_df)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_df[['director','country']] = missing_df[['director','country']].fillna('Unknown')\n",
        "missing_df['cast'] = missing_df['cast'].fillna('No Cast')\n",
        "missing_df['rating'] = missing_df['rating'].fillna(missing_df['rating'].mode()[0])\n",
        "missing_df.dropna(axis=0, inplace = True)\n",
        "missing_df.shape\n"
      ],
      "metadata": {
        "id": "IDcX6D72nrox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code simply performs a check for missing values in the DataFrame named \"missing_df\" and creates a DataFrame named \"missing_value_df\" that shows the number of missing values per column, sorted in descending order.\n",
        "\n",
        "In summary, the missing_value_check function is used to identify columns with missing values, but no missing value imputation techniques are applied in this code."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "outlier_df= missing_df.copy()\n",
        "\n",
        "outlier_df.boxplot(figsize=(15,7))    # As you can see we dont have outliers in the dataset.\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the provided code, a new DataFrame named \"outlier_df\" is created as a copy of the DataFrame \"missing_df.\" Afterward, a box plot is generated for the \"outlier_df\" using the pandas boxplot() function.\n",
        "\n",
        "A box plot is a graphical representation that displays the distribution of data and helps identify potential outliers."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "numerical_df= missing_df.select_dtypes(include='number')\n",
        "non_numerical_df= missing_df.select_dtypes(exclude='number')\n",
        "non_numerical_df.head(2)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the given code, two new DataFrames, namely \"numerical_df\" and \"non_numerical_df,\" are created from the original \"missing_df\" DataFrame.By creating these two distinct DataFrames, the code enables a more focused and tailored approach to analyzing and processing different types of data present in the original \"missing_df\" DataFrame. This segregation is often beneficial in data preprocessing and exploration tasks, where different types of features may require specific treatments to derive meaningful insights and draw valuable conclusions from the dataset.\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "df_tdp = missing_df.copy() # df_tdp stands for datafram for textual data preprocessing\n",
        "\n",
        "# Expand Contraction\n",
        "\n",
        "df_tdp = missing_df[['title']]\n",
        "df_tdp['all_data'] = missing_df['description'] + ' ' + missing_df['listed_in'] + ' ' + missing_df['cast'] + ' ' + missing_df['director'] + ' ' + missing_df['rating']\n",
        "df_tdp.set_index('title', inplace=True)\n",
        "df_tdp.head(5)\n",
        "\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df_tdp['all_data'] = df_tdp['all_data'].str.lower()\n",
        "df_tdp['all_data']\n",
        "df_tdp\n",
        "\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuation(text):\n",
        "    cleaned_text = text.replace(string.punctuation, '')\n",
        "    return cleaned_text\n",
        "\n",
        "df_tdp['all_data'] = df_tdp['all_data'].apply(remove_punctuation)\n",
        "df_tdp.head(5)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "def remove_url_and_numbers(text):\n",
        "    '''This function is used to remove the URL's and Numbers from the given sentence'''\n",
        "    # importing needed libraries\n",
        "    import re\n",
        "    import string\n",
        "\n",
        "    # Replacing the URL's with no space\n",
        "    url_number_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text= re.sub(url_number_pattern,'', text)\n",
        "\n",
        "    # Replacing the digits with one space\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # return the text stripped off URL's and Numbers\n",
        "    return text"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "# extracting the stopwords from nltk library\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "sentences = stopwords.words('english')\n",
        "# displaying the stopwords\n",
        "np.array(sentences)\n",
        "\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White space\n",
        "def remove_stopwords_and_whitespaces(text):\n",
        "    '''This function is used for removing the stopwords from the given sentence'''\n",
        "    text = [word for word in text.split() if not word in stopwords.words('english')]\n",
        "\n",
        "    # joining the list of words with space separator\n",
        "    text=  \" \".join(text)\n",
        "\n",
        "    # removing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # return the manipulated string\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####6. Rephrase Text"
      ],
      "metadata": {
        "id": "AKt9V2Pbzam_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "def stopwords(text):\n",
        "    '''a function for removing the stopword'''\n",
        "    # deleting the stop words and lowering the case of the chosen words\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in st]\n",
        "    # using a space separator to link the list of words\n",
        "    return \" \".join(text)\n"
      ],
      "metadata": {
        "id": "kHT6ZeDyzY9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Downloading needed libraries\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenization\n",
        "# Create a reference variable for Class TweetTokenizer\n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# Importing WordNetLemmatizer from nltk module\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Creating instance for wordnet\n",
        "wordnet  = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizing_sentence(text):\n",
        "    '''This function is used for lemmatizing (changing the given word into meaningfull word) the words from the given sentence'''\n",
        "    text = [wordnet.lemmatize(word) for word in text]\n",
        "\n",
        "    # joining the list of words with space separator\n",
        "    text=  \" \".join(text)\n",
        "\n",
        "    # return the manipulated string\n",
        "    return text"
      ],
      "metadata": {
        "id": "b-k2l4uwiijA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading needed libraries\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Rephrasing text by applying defined lemmatizing function\n",
        "df['description']= df['description'].apply(lemmatizing_sentence)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df.iloc[281,]['description']"
      ],
      "metadata": {
        "id": "PsS2iwYTioL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Lemmatization instead of Stemming for our project because:\n",
        "\n",
        "* Lemmatization produces a more accurate base word: Unlike Stemming, which simply removes the suffix from a word, Lemmatization looks at the meaning of the word and its context to produce a more accurate base form.\n",
        "\n",
        "* Lemmatization can handle different inflections: Lemmatization can handle various inflections of a word, including plural forms, verb tenses, and comparative forms, making it useful for natural language processing.\n",
        "\n",
        "* Lemmatization produces real words: Lemmatization always produces a real word that can be found in a dictionary, making it easier to interpret the results of text analysis.\n",
        "\n",
        "* Lemmatization improves text understanding: By reducing words to their base form, Lemmatization makes it easier to understand the context and meaning of a sentence.\n",
        "\n",
        "* Lemmatization supports multiple languages: While Stemming may only work well for English, Lemmatization is effective for many different languages, making it a more versatile text processing technique."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "# tokenize the text into words before POS Taging\n",
        "df['pos_tags'] = df['description'].apply(nltk.word_tokenize).apply(nltk.pos_tag)\n",
        "\n",
        "# Checking the observation after manipulation\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Importing needed libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Creating instance\n",
        "tfidfv = TfidfVectorizer(max_features=30000)        # Setting max features as 30000 to avoid RAM explosion"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting on TfidfVectorizer\n",
        "x= tfidfv.fit_transform(df['title'])\n",
        "\n",
        "# Checking shape of the formed document matrix\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "yCrqlcpVkNJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used TFIDF vectorization in place of BAG OF WORDS because Tf-idf vectorization takes into account the importance of each word in a document. TF-IDF also assigns higher values to rare words that are unique to a particular document, making them more important in the representation."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Appying TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', lowercase=False, max_features=20000)\n",
        "X = vectorizer.fit_transform(df['title'])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.toarray()[5]"
      ],
      "metadata": {
        "id": "g_hiVsa9mt3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of the vectorized data\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "BjVweBNWmy5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X.toarray()"
      ],
      "metadata": {
        "id": "97QD6jH4m3Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, we believe that dimension reduction is required since it is a statistical approach for lowering the number of random variables in a problem by producing a collection of primary variables."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "# using PCA to reduce dimensionality\n",
        "pca = PCA(random_state=42)\n",
        "pca.fit(X)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explained variance for different number of components\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.title('PCA - Cumulative explained variance vs Number of components')\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.axhline(y= 0.8, color='red', linestyle='--')\n",
        "plt.axvline(x= 3000, color='green', linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QuCJooi0ot7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reducing the dimensions to 0.95 using pca\n",
        "pca = PCA(n_components=3000, random_state=42)\n",
        "pca.fit(X)\n",
        "\n",
        "# transformed features\n",
        "X = pca.transform(X)\n",
        "\n",
        "# shape of transformed vectors\n",
        "X.shape"
      ],
      "metadata": {
        "id": "cUJjDY8woyDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now we are passing the argument so that we can capture 95% of variance.\n",
        "# Defining instance\n",
        "pca_tuned = PCA(n_components=0.95)\n",
        "\n",
        "# Fitting and transforming the model\n",
        "pca_tuned.fit(x.toarray())\n",
        "x_transformed = pca_tuned.transform(x.toarray())\n",
        "\n",
        "# Checking the shape of transformed matrix\n",
        "x_transformed.shape"
      ],
      "metadata": {
        "id": "bU_a6CD4yljO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The technique of lowering the number of features in a dataset while retaining as much useful information as feasible is known as dimensionality reduction. It is an approach for overcoming the curse of dimensionality, which refers to the problem of rising computing complexity and worse performance of machine learning models as the number of features rises.\n",
        "\n",
        "Dimensionality reduction approaches are classified into two types: feature selection and feature extraction.\n",
        "\n",
        "The process of picking a subset of the most relevant characteristics from the original feature set is known as feature selection. It is a strategy for reducing data dimensionality by deleting unnecessary and superfluous characteristics. The following are examples of common feature selection techniques:\n",
        "\n",
        "Feature selection based on correlation Feature selection based on mutual information Recursive feature removal SelectKBest The technique of extracting additional features from an existing feature set by combining or altering existing features is known as feature extraction. It is a technique that aids in data reduction by generating a new feature space that is more compact and informative than the original feature space. Techniques for extracting features that are often used include:\n",
        "\n",
        "PCA stands for Principal Component Analysis. LDA stands for Linear Discriminant Analysis. Non-Negative Matrix Factorization (NMF) Independent Component Analysis (ICA) Answer Here for Autoencoder."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***8"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 (K-Means Clustering)"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-means clustering is a type of unsupervised machine learning algorithm used for partitioning a dataset into K clusters based on similarity of data points. The goal of the algorithm is to minimize the sum of squared distances between each data point and its corresponding cluster centroid. It works iteratively by assigning each data point to its nearest centroid and then re-computing the centroid of each cluster based on the new assignments. The algorithm terminates when the cluster assignments no longer change or when a maximum number of iterations is reached.\n",
        "\n",
        "Let's just itterate over a loop of 1 to 16 clusters and try to find the optimal number of clusters with ELBOW method."
      ],
      "metadata": {
        "id": "K1uHFydJuBm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "## Determining optimal value of K using KElbowVisualizer\n",
        "# Importing needed library\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# Elbow method to find the optimal value of k\n",
        "wcss=[]\n",
        "for i in range(1,31):\n",
        "  kmeans = KMeans(n_clusters=i,init='k-means++',random_state=33)\n",
        "  kmeans.fit(x)\n",
        "  wcss_iter = kmeans.inertia_\n",
        "  wcss.append(wcss_iter)\n",
        "\n",
        "number_clusters = range(1,31)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(number_clusters,wcss)\n",
        "plt.title('The Elbow Method - KMeans clustering')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here it seems that the elbow is forming at the 2 clusters but before blindly believing it let's plot one more chart that itterates over the same number of cluters and determines the Silhouette Score at every point.\n",
        "\n",
        "Okay, but what is Silhouette Score?\n",
        "\n",
        "The silhouette score is a measure of how similar an object is to its own cluster compared to other clusters. It is used to evaluate the quality of clustering, where a higher score indicates that objects are more similar to their own cluster and dissimilar to other clusters.\n",
        "\n",
        "The silhouette score ranges from -1 to 1, where a score of 1 indicates that the object is well-matched to its own cluster, and poorly-matched to neighboring clusters. Conversely, a score of -1 indicates that the object is poorly-matched to its own cluster, and well-matched to neighboring clusters."
      ],
      "metadata": {
        "id": "Ynk4Ix03ulmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Determining optimal value of K using KElbowVisualizer\n",
        "# Importing needed library\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "# Instantiate the clustering model and visualizer\n",
        "visualizer = KElbowVisualizer(model, k=(2,16), metric='silhouette', timings=True, locate_elbow=False)\n",
        "\n",
        "# Fit the data to the visualizer\n",
        "visualizer.fit(x)\n",
        "\n",
        "# Finalize and render the figure\n",
        "visualizer.show()"
      ],
      "metadata": {
        "id": "CCkGsNMKuq8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Computing Silhouette score for each k\n",
        "# Importing needed libraries\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Defining Range\n",
        "k_range = range(2, 7)\n",
        "for k in k_range:\n",
        "    Kmodel = KMeans(n_clusters=k)\n",
        "    labels = Kmodel.fit_predict(x)\n",
        "    score = silhouette_score(x, labels)\n",
        "    print(\"k=%d, Silhouette score=%f\" % (k, score))"
      ],
      "metadata": {
        "id": "OkkZILmNuyO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plots (Elbow plot and Sillhouette plot) it is very clear that the Silhoutte score is comparatively good for 4 number of clusters, so we will consider 4 cluster in kmeans analysis.\n"
      ],
      "metadata": {
        "id": "zOYFxgGgu3RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cluster values to the dateframe.\n",
        "df['kmeans_cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "gRRSbGWMu91s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting with defining a function that plot a wordcloud for each of the attribute in the given dataframe."
      ],
      "metadata": {
        "id": "8CBK9Nyhv5P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "def kmeans_wordcloud(cluster_number, column_name):\n",
        "    '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "    #Importing libraries\n",
        "    from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "    # Filter the data by the specified cluster number and column name\n",
        "    df_wordcloud = df[['kmeans_cluster', column_name]].dropna()\n",
        "    df_wordcloud = df_wordcloud[df_wordcloud['kmeans_cluster'] == cluster_number]\n",
        "    df_wordcloud = df_wordcloud[df_wordcloud[column_name].str.len() > 0]\n",
        "\n",
        "    # Combine all text documents into a single string\n",
        "    text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "    # Create the word cloud\n",
        "    wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"black\").generate(text)\n",
        "\n",
        "    # Convert the wordcloud to a numpy array\n",
        "    image_array = wordcloud.to_array()\n",
        "\n",
        "    # Return the numpy array\n",
        "    return image_array"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Implementing the above defined function and plotting the wordcloud of each attribute\n",
        "fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(20, 15))\n",
        "for i in range(4):\n",
        "    for j, col in enumerate(['description', 'listed_in', 'country', 'title']):\n",
        "        axs[j][i].imshow(kmeans_wordcloud(i, col))\n",
        "        axs[j][i].axis('off')\n",
        "        axs[j][i].set_title(f'Cluster {i}, {col}',fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eJF85qgM9l66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 (Hierarchial Clustering)"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Hierarchical clustering is a type of clustering algorithm used for grouping similar data points together into clusters based on their similarity, by recursively merging or dividing clusters based on a measure of similarity or distance between them.\n",
        "\n",
        "Let's dive into it by plotting a Dendogram and then we will determine the optimal number of clusters."
      ],
      "metadata": {
        "id": "m_Bk59B7wczO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing needed libraries\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import scipy.cluster.hierarchy as shc\n",
        "\n",
        "# Building a dendogram to decide on the number of clusters\n",
        "plt.figure(figsize =(15, 8))\n",
        "plt.title('Visualising the data')\n",
        "Dendrogram = shc.dendrogram((shc.linkage(X, method ='ward')))"
      ],
      "metadata": {
        "id": "t1ke6OGU6whs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, but what is Dendogram and how to determine the optimal value of clusters?\n",
        "\n",
        "A dendrogram is a tree-like diagram that records the sequences of merges or splits.More the distance of the vertical lines in the dendrogram, more the distance between those clusters.\n",
        "From the above Dendogram we can say that optimal value of clusters is 2. But before assigning the vlaues to respective clusters, let's check the silhouette scores using Agglomerative clustering and follow the bottom up approach to aggregate the datapoints.\n"
      ],
      "metadata": {
        "id": "-cQm7K6x64AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Computing Silhouette score for each k\n",
        "# Importing needed libraries\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Range selected from dendrogram above\n",
        "k_range = range(2, 10)\n",
        "for k in k_range:\n",
        "    model = AgglomerativeClustering(n_clusters=k)\n",
        "    labels = model.fit_predict(x_transformed)\n",
        "    score = silhouette_score(x, labels)\n",
        "    print(\"k=%d, Silhouette score=%f\" % (k, score))"
      ],
      "metadata": {
        "id": "NDsbw380xKCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above silhouette scores it is clear that the 2 clusters are optimal value (maximum Silhouette score), which is also clear from the above Dendogram that for 2 cluters the euclidean distances are maximum.\n",
        "\n",
        "Let's again plot the chart and observe the 2 different formed clusters."
      ],
      "metadata": {
        "id": "workJa_b7E97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the K-means model on a dataset\n",
        "Agmodel = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "\n",
        "#predict the labels of clusters.\n",
        "plt.figure(figsize=(10,6), dpi=120)\n",
        "label = Agmodel.fit_predict(x_transformed)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(x_transformed[label == i , 0] , x_transformed[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xB-rYLrM7Vi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again plotting the 3 Dimensional plot to see the clusters clearly.\n"
      ],
      "metadata": {
        "id": "s1J43oUY8pQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing library to visualize clusters in 3D\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Plot the clusters in 3D\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "colors = ['r', 'g', 'b', 'y']\n",
        "for i in range(len(colors)):\n",
        "    ax.scatter(x_transformed[Agmodel.labels_ == i, 0], x_transformed[Agmodel.labels_ == i, 1], x_transformed[Agmodel.labels_ == i, 2],c=colors[i])\n",
        "ax.set_xlabel('x-axis')\n",
        "ax.set_ylabel('y-axis')\n",
        "ax.set_zlabel('z-axis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4_O8LkWE8tUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, we can again easily differentiate the all 2 clusters with naked eye. Now let's assign the 'Content(Movies and TV Shows)' in their respective cluster by appending 1 more attribute in the final dataframe."
      ],
      "metadata": {
        "id": "hW2bgrdS8x8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cluster values to the dateframe.\n",
        "df['agglomerative_cluster'] = Agmodel.labels_"
      ],
      "metadata": {
        "id": "485tnd4Y817b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's just again define a function that plots wordcloud for different attributes using Agglomerative Clustering"
      ],
      "metadata": {
        "id": "be30b6jU8_6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "def agglomerative_wordcloud(cluster_number, column_name):\n",
        "  '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "  #Importing libraries\n",
        "  from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "  # Filter the data by the specified cluster number and column name\n",
        "  df_wordcloud = df[['agglomerative_cluster', column_name]].dropna()\n",
        "  df_wordcloud = df_wordcloud[df_wordcloud['agglomerative_cluster'] == cluster_number]\n",
        "\n",
        "  # Combine all text documents into a single string\n",
        "  text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "  # Create the word cloud\n",
        "  wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"black\").generate(text)\n",
        "\n",
        "  # Return the word cloud object\n",
        "  return wordcloud\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the above defined function and plotting the wordcloud of each attribute\n",
        "fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(20, 15))\n",
        "for i in range(2):\n",
        "    for j, col in enumerate(['description', 'listed_in', 'country', 'title']):\n",
        "        axs[j][i].imshow(agglomerative_wordcloud(i, col))\n",
        "        axs[j][i].axis('off')\n",
        "        axs[j][i].set_title(f'Cluster {i}, {col}',fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1HnFhXVn9UoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 (Building a Recommendaton System)"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using Cosine similarity as it is a measure of similarity between two non-zero vectors in a multidimensional space. It measures the cosine of the angle between the two vectors, which ranges from -1 (opposite direction) to 1 (same direction), with 0 indicating orthogonality (the vectors are perpendicular to each other).\n",
        "\n",
        "In this project we have used cosine similarity which is used to determine how similar two documents or pieces of text are. We represent the documents as vectors in a high-dimensional space, where each dimension represents a word or term in the corpus. We can then calculate the cosine similarity between the vectors to determine how similar the documents are based on their word usage.\n",
        "\n",
        "We are using cosine similarity over tf-idf because:\n",
        "\n",
        "* Cosine similarity handles high dimensional sparse data better.\n",
        "\n",
        "* Cosine similarity captures the meaning of the text better than tf-idf. For example, if two items contain similar words but in different orders, cosine similarity would still consider them similar, while tf-idf may not. This is because tf-idf only considers the frequency of words in a document and not their order or meaning."
      ],
      "metadata": {
        "id": "F3BH-oZn-gt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Importing neede libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Create a TF-IDF vectorizer object and transform the text data\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(df['title'])\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "def recommend_content(title, cosine_sim=cosine_sim, data=df):\n",
        "    # Get the index of the input title in the programme_list\n",
        "    programme_list = data['title'].to_list()\n",
        "    index = programme_list.index(title)\n",
        "\n",
        "    # Create a list of tuples containing the similarity score and index\n",
        "    # between the input title and all other programmes in the dataset\n",
        "    sim_scores = list(enumerate(cosine_sim[index]))\n",
        "\n",
        "    # Sort the list of tuples by similarity score in descending order\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:11]\n",
        "\n",
        "    # Get the recommended movie titles and their similarity scores\n",
        "    recommend_index = [i[0] for i in sim_scores]\n",
        "    rec_movie = data['title'].iloc[recommend_index]\n",
        "    rec_score = [round(i[1], 4) for i in sim_scores]\n",
        "\n",
        "    # Create a pandas DataFrame to display the recommendations\n",
        "    rec_table = pd.DataFrame(list(zip(rec_movie, rec_score)), columns=['Recommendation', 'Similarity_score(0-1)'])\n",
        "\n",
        "    return rec_table"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check how our recommender system is performing."
      ],
      "metadata": {
        "id": "SrTOqNfc_Gq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing indian movie\n",
        "recommend_content('Kal Ho Naa Ho')"
      ],
      "metadata": {
        "id": "F0IlQ8os_KYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing non indian movie\n",
        "recommend_content('Zombieland')"
      ],
      "metadata": {
        "id": "apYPbFX-_Nwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing indian tv show\n",
        "recommend_content('Zindagi Gulzar Hai')"
      ],
      "metadata": {
        "id": "Jv_1L-RD_RdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing non indian tv show\n",
        "recommend_content('Vampires')"
      ],
      "metadata": {
        "id": "1YgzPb7m_VEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have chosen Silhoutte Score over Distortion Score (also known as inertia or sum of squared distances) as evaluation metrics as it measures how well each data point in a cluster is separated from other clusters. It ranges from -1 to 1, with higher values indicating better cluster separation. A silhouette score close to 1 indicates that the data point is well-matched to its own cluster and poorly matched to neighboring clusters. A score close to 0 indicates that the data point is on or very close to the boundary between two clusters. A score close to -1 indicates that the data point is probably assigned to the wrong cluster.\n",
        "\n",
        "The advantages of using silhouette score over distortion score are:\n",
        "\n",
        "* Silhouette score takes into account both the cohesion (how well data points within a cluster are similar) and separation (how well data points in different clusters are dissimilar) of the clusters, whereas distortion score only considers the compactness of each cluster.\n",
        "* Silhouette score is less sensitive to the shape of the clusters, while distortion score tends to favor spherical clusters, and in our case the clusters are not completely spherical.\n",
        "* Silhouette score provides more intuitive and interpretable results, as it assigns a score to each data point rather than just a single value for the entire clustering solution."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have considered K-means as our final model, as we are getting the comparatevely high Silhoutte Score in K-means clustering and the resulted clusters are very well seperated from each others as have saw in the 3 dimensions.\n",
        "\n",
        "Also in some of the situations K-means works more accurately then other clustering methods such as:\n",
        "\n",
        "\n",
        "* **Speed:** K-means is generally faster than hierarchical clustering, especially when dealing with large datasets, since it involves fewer calculations and iterations.\n",
        "\n",
        "* **Ease of use: **K-means is relatively straightforward to implement and interpret, as it requires only a few parameters (such as the number of clusters) and produces a clear partitioning of the data.\n",
        "\n",
        "* **Scalability:** K-means can easily handle datasets with a large number of variables or dimensions, whereas hierarchical clustering becomes computationally expensive as the number of data points and dimensions increase.\n",
        "\n",
        "* **Independence of clusters:** K-means produces non-overlapping clusters, whereas hierarchical clustering can produce overlapping clusters or clusters that are nested within each other, which may not be ideal for certain applications."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusions drawn from EDA**"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Based on the exploratory data analysis (EDA) of the Netflix movies and TV shows clustering dataset, we have drawn the following conclusions:**\n",
        "\n",
        "* Movies make up about two-thirds *of Netflix content, with TV shows comprising the remaining *one-third.\n",
        "\n",
        "* Adult and teen categories are prevalent on Netflix, while family-friendly content is more common in TV shows than in movies.\n",
        "\n",
        "* Indian actors dominate Netflix movies, while popular Indian actors are absent from TV shows.\n",
        "\n",
        "* Jan Suter is the most common movie director, and Ken Burns is the most common TV show director on Netflix.\n",
        "\n",
        "* The United States is the largest producer of movies and TV shows on Netflix, followed by India. Japan and South Korea have more TV shows than movies, indicating growth potential in that area.\n",
        "\n",
        "* International movies, drama, and comedy are the most popular genres on Netflix.\n",
        "\n",
        "* TV show additions on Netflix have increased since 2018, while movie additions have decreased. In 2020, fewer movies were added compared to 2019, but more TV shows were added.\n",
        "\n",
        "* October, November, and December are popular months for adding TV shows, while January, October, and November are popular for adding movies. February sees the least additions.\n",
        "\n",
        "* Movies and TV shows are typically added at the beginning or middle of the month and are popularly added on weekends.\n",
        "\n",
        "* Most movies on Netflix have durations between 80 to 120 minutes, while TV shows commonly have one or two seasons.\n",
        "\n",
        "* Various countries contribute adult and teen content, with Spain producing the most adult content and Canada focusing on children and family-friendly categories."
      ],
      "metadata": {
        "id": "00hYx2OZAe29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusions drawn from ML Model**"
      ],
      "metadata": {
        "id": "o5OfNhh-A8e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implemented K-Means Clustering and Agglomerative Hierarchical Clustering, to cluster the Netflix Movies TV show dataset.\n",
        "* The optimal number of clusters we are getting from K-means is 4, whereas for Agglomerative Hierarchical Clustering the optimal number of clusters are found out to be 2.\n",
        "* We chose Silhouette Score as the evaluation metric over distortion score because it provides a more intuitive and interpretable result. Also Silhouette score is less sensitive to the shape of the clusters.\n",
        "* Built a Recommendation system that can help Netflix improve user experience and reduce subscriber churn by providing personalized recommendations to users based on their similarity scores."
      ],
      "metadata": {
        "id": "8x6auWcNBFQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}